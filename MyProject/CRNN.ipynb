{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "N6-KPHZ91zNI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f3fc5e1-aa93-4f99-ff5c-141a3ec32e6f"
      },
      "source": [
        "%tensorflow_version 1.x"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-8d2919c1d33c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tensorflow_version'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'1.x'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_line_magic\u001b[0;34m(self, magic_name, line, _stack_depth)\u001b[0m\n\u001b[1;32m   2312\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'local_ns'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_locals\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2313\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2314\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2315\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2316\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/_tensorflow_magics.py\u001b[0m in \u001b[0;36m_tensorflow_version\u001b[0;34m(line)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m              \u001b[0mYour\u001b[0m \u001b[0mnotebook\u001b[0m \u001b[0mshould\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mupdated\u001b[0m \u001b[0mto\u001b[0m \u001b[0muse\u001b[0m \u001b[0mTensorflow\u001b[0m \u001b[0;36m2.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m              \u001b[0mSee\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mguide\u001b[0m \u001b[0mat\u001b[0m \u001b[0mhttps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0mwww\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morg\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mguide\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mmigrate\u001b[0m\u001b[0;31m#migrate-from-tensorflow-1x-to-tensorflow-2.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m                        ))\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Tensorflow 1 is unsupported in Colab.\n\nYour notebook should be updated to use Tensorflow 2.\nSee the guide at https://www.tensorflow.org/guide/migrate#migrate-from-tensorflow-1x-to-tensorflow-2."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xq-IHfAQ15A7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac04383c-4b0a-4a94-f686-5c53e7d1a0b0"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/gdrive/MyDrive/Colab Notebooks/CRNN_DataSet_Generartor"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EF8dOm49v1zE",
        "outputId": "378c2d8e-eb12-453f-bbd6-aa2809cf118f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/MyDrive/Colab Notebooks/CRNN_DataSet_Generartor\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YYT7zH6n394M"
      },
      "source": [
        "!mkdir images/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bL_25u1_4DnB"
      },
      "source": [
        "!python generate_data.py --n_samples 3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install h5py==2.10.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dH-NJ-Hsn5E3",
        "outputId": "ec4f230f-4342-48da-dc3f-cbb8c33f1eee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting h5py==2.10.0\n",
            "  Downloading h5py-2.10.0-cp37-cp37m-manylinux1_x86_64.whl (2.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9 MB 5.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from h5py==2.10.0) (1.15.0)\n",
            "Requirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.7/dist-packages (from h5py==2.10.0) (1.21.6)\n",
            "Installing collected packages: h5py\n",
            "  Attempting uninstall: h5py\n",
            "    Found existing installation: h5py 3.1.0\n",
            "    Uninstalling h5py-3.1.0:\n",
            "      Successfully uninstalled h5py-3.1.0\n",
            "Successfully installed h5py-2.10.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UwcIFwjg4IN-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e498a6f-4945-481d-e5c5-9024252f4850"
      },
      "source": [
        "import fnmatch\n",
        "import cv2\n",
        "import numpy as np\n",
        "import string\n",
        "import time\n",
        "\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "from keras.layers import Dense, LSTM, Reshape, BatchNormalization, Input, Conv2D, MaxPool2D, Lambda, Bidirectional\n",
        "from keras.models import Model\n",
        "from keras.activations import relu, sigmoid, softmax\n",
        "import keras.backend as K\n",
        "from keras.utils import to_categorical\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "import os\n",
        "import tensorflow as tf\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "from sklearn.utils import shuffle\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using TensorFlow backend.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oNmZv6iY4Loh"
      },
      "source": [
        "char_list = string.ascii_letters+string.digits\n",
        " \n",
        "def encode_to_labels(txt):\n",
        "    # encoding each output word into digits\n",
        "    dig_lst = []\n",
        "    for index, char in enumerate(txt):\n",
        "        try:\n",
        "            dig_lst.append(char_list.index(char))\n",
        "        except:\n",
        "            print(char)\n",
        "        \n",
        "    return dig_lst\n",
        "\n",
        "def find_dominant_color(image):\n",
        "        #Resizing parameters\n",
        "        width, height = 150,150\n",
        "        image = image.resize((width, height),resample = 0)\n",
        "        #Get colors from image object\n",
        "        pixels = image.getcolors(width * height)\n",
        "        #Sort them by count number(first element of tuple)\n",
        "        sorted_pixels = sorted(pixels, key=lambda t: t[0])\n",
        "        #Get the most frequent color\n",
        "        dominant_color = sorted_pixels[-1][1]\n",
        "        return dominant_color\n",
        "\n",
        "def preprocess_img(img, imgSize):\n",
        "    \"put img into target img of size imgSize, transpose for TF and normalize gray-values\"\n",
        "\n",
        "    # there are damaged files in IAM dataset - just use black image instead\n",
        "    if img is None:\n",
        "        img = np.zeros([imgSize[1], imgSize[0]]) \n",
        "        print(\"Image None!\")\n",
        "\n",
        "    # create target image and copy sample image into it\n",
        "    (wt, ht) = imgSize\n",
        "    (h, w) = img.shape\n",
        "    fx = w / wt\n",
        "    fy = h / ht\n",
        "    f = max(fx, fy)\n",
        "    newSize = (max(min(wt, int(w / f)), 1),\n",
        "               max(min(ht, int(h / f)), 1))  # scale according to f (result at least 1 and at most wt or ht)\n",
        "    img = cv2.resize(img, newSize, interpolation=cv2.INTER_CUBIC) # INTER_CUBIC interpolation best approximate the pixels image\n",
        "                                                               # see this https://stackoverflow.com/a/57503843/7338066\n",
        "    most_freq_pixel=find_dominant_color(Image.fromarray(img))\n",
        "    target = np.ones([ht, wt]) * most_freq_pixel  \n",
        "    target[0:newSize[1], 0:newSize[0]] = img\n",
        "\n",
        "    img = target\n",
        "\n",
        "    return img\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sic3Urxs4OkE"
      },
      "source": [
        "\n",
        "training_img = []\n",
        "training_txt = []\n",
        "train_input_length = []\n",
        "train_label_length = []\n",
        "orig_txt = []\n",
        " \n",
        "#lists for validation dataset\n",
        "valid_img = []\n",
        "valid_txt = []\n",
        "valid_input_length = []\n",
        "valid_label_length = []\n",
        "valid_orig_txt = []\n",
        " \n",
        "max_label_len = 0\n",
        "\n",
        "annot=open('/content/Data-generator-for-CRNN/annotation.txt','r').readlines()\n",
        "imagenames=[]\n",
        "txts=[]\n",
        "\n",
        "for cnt in annot:\n",
        "    filename,txt=cnt.split(',')[0],cnt.split(',')[1].split('\\n')[0]\n",
        "    imagenames.append(filename)\n",
        "    txts.append(txt)\n",
        "    \n",
        "c = list(zip(imagenames, txts))\n",
        "\n",
        "random.shuffle(c)\n",
        "\n",
        "imagenames, txts = zip(*c)\n",
        "    \n",
        "\n",
        "    \n",
        "for i in range(len(imagenames)):\n",
        "        img = cv2.imread('/content/Data-generator-for-CRNN/images/'+imagenames[i],0)   \n",
        " \n",
        "        img=preprocess_img(img,(128,32))\n",
        "        img=np.expand_dims(img,axis=-1)\n",
        "        img = img/255.\n",
        "        txt = txts[i]\n",
        "        \n",
        "        # compute maximum length of the text\n",
        "        if len(txt) > max_label_len:\n",
        "            max_label_len = len(txt)\n",
        "            \n",
        "           \n",
        "        # split the 150000 data into validation and training dataset as 10% and 90% respectively\n",
        "        if i%10 == 0:     \n",
        "            valid_orig_txt.append(txt)   \n",
        "            valid_label_length.append(len(txt))\n",
        "            valid_input_length.append(31)\n",
        "            valid_img.append(img)\n",
        "            valid_txt.append(encode_to_labels(txt))\n",
        "        else:\n",
        "            orig_txt.append(txt)   \n",
        "            train_label_length.append(len(txt))\n",
        "            train_input_length.append(31)\n",
        "            training_img.append(img)\n",
        "            training_txt.append(encode_to_labels(txt)) \n",
        "        \n",
        "        # break the loop if total data is 150000\n",
        "        if i == 150000:\n",
        "            flag = 1\n",
        "            break\n",
        "        i+=1\n",
        "        \n",
        "    \n",
        "    \n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kp-U2u0p4Y_T"
      },
      "source": [
        "#pad each output label to maximum text length\n",
        " \n",
        "train_padded_txt = pad_sequences(training_txt, maxlen=max_label_len, padding='post', value = len(char_list))\n",
        "valid_padded_txt = pad_sequences(valid_txt, maxlen=max_label_len, padding='post', value = len(char_list))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SqWcDplF6HAt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ebd3895-364a-4ba2-e127-e039d81cf921"
      },
      "source": [
        "inputs = Input(shape=(32,128,1))\n",
        " \n",
        "# convolution layer with kernel size (3,3)\n",
        "conv_1 = Conv2D(64, (3,3), activation = 'relu', padding='same')(inputs)\n",
        "# poolig layer with kernel size (2,2)\n",
        "pool_1 = MaxPool2D(pool_size=(2, 2), strides=2)(conv_1)\n",
        " \n",
        "conv_2 = Conv2D(128, (3,3), activation = 'relu', padding='same')(pool_1)\n",
        "pool_2 = MaxPool2D(pool_size=(2, 2), strides=2)(conv_2)\n",
        " \n",
        "conv_3 = Conv2D(256, (3,3), activation = 'relu', padding='same')(pool_2)\n",
        " \n",
        "conv_4 = Conv2D(256, (3,3), activation = 'relu', padding='same')(conv_3)\n",
        "# poolig layer with kernel size (2,1)\n",
        "pool_4 = MaxPool2D(pool_size=(2, 1))(conv_4)\n",
        " \n",
        "conv_5 = Conv2D(512, (3,3), activation = 'relu', padding='same')(pool_4)\n",
        "# Batch normalization layer\n",
        "batch_norm_5 = BatchNormalization()(conv_5)\n",
        " \n",
        "conv_6 = Conv2D(512, (3,3), activation = 'relu', padding='same')(batch_norm_5)\n",
        "batch_norm_6 = BatchNormalization()(conv_6)\n",
        "pool_6 = MaxPool2D(pool_size=(2, 1))(batch_norm_6)\n",
        "\n",
        " \n",
        "conv_7 = Conv2D(512, (2,2), activation = 'relu')(pool_6)\n",
        " \n",
        "squeezed = Lambda(lambda x: K.squeeze(x, 1))(conv_7)\n",
        "\n",
        "# bidirectional LSTM layers with units=128\n",
        "blstm_1 = Bidirectional(LSTM(128, return_sequences=True, dropout = 0.2))(squeezed)\n",
        "blstm_2 = Bidirectional(LSTM(128, return_sequences=True, dropout = 0.2))(blstm_1)\n",
        " \n",
        "outputs = Dense(len(char_list)+1, activation = 'softmax')(blstm_2)\n",
        "\n",
        "# model to be used at test time\n",
        "act_model = Model(inputs, outputs)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/keras/backend/tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jBtmnGNn6I0J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20213623-9f06-4f81-fac3-af4b78d7673a"
      },
      "source": [
        "act_model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         (None, 32, 128, 1)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 32, 128, 64)       640       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 16, 64, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 16, 64, 128)       73856     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 8, 32, 128)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 8, 32, 256)        295168    \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 8, 32, 256)        590080    \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 4, 32, 256)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 4, 32, 512)        1180160   \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 4, 32, 512)        2048      \n",
            "_________________________________________________________________\n",
            "conv2d_6 (Conv2D)            (None, 4, 32, 512)        2359808   \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 4, 32, 512)        2048      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2 (None, 2, 32, 512)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_7 (Conv2D)            (None, 1, 31, 512)        1049088   \n",
            "_________________________________________________________________\n",
            "lambda_1 (Lambda)            (None, 31, 512)           0         \n",
            "_________________________________________________________________\n",
            "bidirectional_1 (Bidirection (None, 31, 256)           656384    \n",
            "_________________________________________________________________\n",
            "bidirectional_2 (Bidirection (None, 31, 256)           394240    \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 31, 63)            16191     \n",
            "=================================================================\n",
            "Total params: 6,619,711\n",
            "Trainable params: 6,617,663\n",
            "Non-trainable params: 2,048\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TMxpZweM6MfI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ab1ec5c-a269-45f4-8ea2-a27ede93d291"
      },
      "source": [
        "labels = Input(name='the_labels', shape=[max_label_len], dtype='float32')\n",
        "input_length = Input(name='input_length', shape=[1], dtype='int64')\n",
        "label_length = Input(name='label_length', shape=[1], dtype='int64')\n",
        " \n",
        " \n",
        "def ctc_lambda_func(args):\n",
        "    y_pred, labels, input_length, label_length = args\n",
        " \n",
        "    return K.ctc_batch_cost(labels, y_pred, input_length, label_length)\n",
        " \n",
        " \n",
        "loss_out = Lambda(ctc_lambda_func, output_shape=(1,), name='ctc')([outputs, labels, input_length, label_length])\n",
        "\n",
        "#model to be used at training time\n",
        "model = Model(inputs=[inputs, labels, input_length, label_length], outputs=loss_out)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/array_ops.py:1475: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xhqs_1FE6NYR"
      },
      "source": [
        "model.compile(loss={'ctc': lambda y_true, y_pred: y_pred}, optimizer = 'adam')\n",
        " \n",
        "filepath=\"/content/CRNN_model.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath=filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='auto')\n",
        "callbacks_list = [checkpoint]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2LhU9Lpj6QU-"
      },
      "source": [
        "training_img = np.array(training_img)\n",
        "train_input_length = np.array(train_input_length)\n",
        "train_label_length = np.array(train_label_length)\n",
        "\n",
        "valid_img = np.array(valid_img)\n",
        "valid_input_length = np.array(valid_input_length)\n",
        "valid_label_length = np.array(valid_label_length)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bx41yVYP9tQh"
      },
      "source": [
        "batch_size = 256\n",
        "epochs = 15\n",
        "model.fit(x=[training_img, train_padded_txt, train_input_length, train_label_length], y=np.zeros(len(training_img)), batch_size=batch_size, epochs = epochs, validation_data = ([valid_img, valid_padded_txt, valid_input_length, valid_label_length], [np.zeros(len(valid_img))]), verbose = 1, callbacks = callbacks_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pW4mPMB3MpCF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c138142-d66b-422c-bccf-96d5bde31d99"
      },
      "source": [
        "# load the saved best model weights\n",
        "act_model.load_weights('/content/CRNN_model.hdf5')\n",
        " \n",
        "# predict outputs on validation images\n",
        "prediction = act_model.predict(valid_img[10:20])\n",
        " \n",
        "# use CTC decoder\n",
        "out = K.get_value(K.ctc_decode(prediction, input_length=np.ones(prediction.shape[0])*prediction.shape[1],\n",
        "                         greedy=True)[0][0])\n",
        " \n",
        "# see the results\n",
        "i = 10\n",
        "for x in out:\n",
        "    print(\"original_text =  \", valid_orig_txt[i])\n",
        "    print(\"predicted text = \", end = '')\n",
        "    for p in x:  \n",
        "        if int(p) != -1:\n",
        "            print(char_list[int(p)], end = '')       \n",
        "    print('\\n')\n",
        "    i+=1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "original_text =   bbnr\n",
            "predicted text = bbnr\n",
            "\n",
            "original_text =   weheoquxfog\n",
            "predicted text = weheoquxfog\n",
            "\n",
            "original_text =   pfwitcsotl\n",
            "predicted text = pfwitcsotl\n",
            "\n",
            "original_text =   icvualtratnewxwmofwh\n",
            "predicted text = tcvualtratnewxwmofwh\n",
            "\n",
            "original_text =   nz\n",
            "predicted text = nz\n",
            "\n",
            "original_text =   hnxvk\n",
            "predicted text = hnxvk\n",
            "\n",
            "original_text =   tm\n",
            "predicted text = tm\n",
            "\n",
            "original_text =   erkpjznpioakowxlcaxl\n",
            "predicted text = erkpjznptoakowxlcaxl\n",
            "\n",
            "original_text =   xmweyjs\n",
            "predicted text = xmweyjs\n",
            "\n",
            "original_text =   rcbbjuyxqtdnp\n",
            "predicted text = rcbbjuyxqtdnp\n",
            "\n"
          ]
        }
      ]
    }
  ]
}